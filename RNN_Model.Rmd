---
title: "RNN_Model"
author: "Handing Zhang"
date: "4/25/2022"
output: pdf_document
---
Here I will fit a RNN model 

```{r Import Data. Load Packages}
source("Data_Wrangling.R")
pacman::p_load(ISLR2,keras,tidyverse,magrittr,ggplot2)

Z_416 <- Z_416 %>% select(-c(Y1, Y2))
```


```{r}
library(ISLR2)
library(magrittr)
library(keras)
xdata <- data.matrix(
 NYSE[, c("DJ_return", "log_volume","log_volatility")]
 )

istrain <- NYSE[, "train"]
xdata <- scale(xdata)

```


We first create lagged versions of the time series.  We start with a function that takes as input a data
matrix and a lag $L$, and returns a lagged version of the matrix. It
simply inserts $L$ rows of `NA` at the top, and truncates the
bottom.

```{r, a function that takes dataframe x and spit it with k rows of lags. (Intuitively the dataframe moved downwards by k steps with NA rows as its foot prints.)}
lagm <- function(x, k = 1) {
   n <- nrow(x)
   pad <- matrix(NA, k, ncol(x))
   colnames(pad) <- colnames(x)
   rbind(pad, x[1:(n - k), ])
}


# To Help understanding ####
# a <- matrix(c(rep(1,6), rep(2,6), rep(3,6), rep(4, 6), rep(5, 6), rep(6,6) ), nrow = 6, byrow = T)
# a
# lagm(a,2)
# data.frame(c(),a,a)
# aaaaa <- data.frame(letters[1:6],
#    L1 = lagm(a, 1), L2 = lagm(a, 2),
#    L3 = lagm(a, 3), L4 = lagm(a, 4),
#    L5 = lagm(a, 5)
#  )
# aaaaa
# a
```

We now use this function to create a data frame with all the required
lags, as well as the response variable.

```{r a Function that combines L dataframes and our response Y. Each dataframe i takes i lags(defined from the function above.)}

# Make sure the input dataframe contains column "Y", our response!
lag_gene <- function(df, L){
  temp <- data.frame(Y = df[,"Y"])
  for(i in c(1:L)){
    # name <- paste0("L", i)
    # assign(name, lagm(df, i))
    temp <- data.frame(temp, lagm(df, i)[,-1])
  }
  return(temp)
}

lag_gene(Z_416, L = 6)
```


```{r create a list containing data frame with different number of steps of lags. We choose the range of 1 to 100 lags}

lag_list_gene <- function(df){
  Lag_List <- list()
  for (i in c(1:100)){
  Lag_List[i] <- lag_gene(df, i)
  }
  return(Lag_List)
}
lag_gene(Z_416,1)
lag_list_gene(Z_416)
```



```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"],
   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
   L5 = lagm(xdata, 5)
 )
arframe
```


If we look at the first five rows of this frame, we will see some
missing values in the lagged variables (due to the construction above). We remove these rows, and adjust `istrain`
accordingly.

```{r}
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```

We now fit the linear AR model to the training data using `lm()`, and predict on the
test data.

```{r}
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
```

The last two lines compute the $R^2$ on the test data, as defined in (3.17).

We refit this model, including the factor variable `day_of_week`.

```{r}
nrow(NYSE)
nrow(arframe)
arframed <-
    data.frame(day = NYSE[-c(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```

To fit the RNN, we need to reshape these data, since it expects a
sequence of $L=5$ feature vectors $X=\{X_\ell\}_1^L$ for each observation, as in (10.20) on
page  428. These are  lagged versions of the
time series going back $L$ time points.

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)
```

We have done this in four steps. The first simply extracts the
$n\times 15$ matrix of lagged versions of the three predictor
variables from `arframe`. The second converts this matrix to a
$n\times 3\times 5$ array. We can do this by simply changing the
dimension attribute, since the new array is filled column wise. The
third step reverses the order of lagged variables, so that index $1$
is furthest back in time, and index $5$ closest. The
final step rearranges the coordinates of the array (like a partial
transpose) into the format that the RNN module in `keras`
expects.

Now we are ready to proceed with the RNN, which uses 12 hidden units.

```{r}

model <- keras_model_sequential() %>%
   layer_simple_rnn(units = 12,
      input_shape = list(5, 3),
      dropout = 0.1, recurrent_dropout = 0.1) %>%
   layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
    loss = "mse")
```

We specify two forms of dropout for the units feeding into the  hidden
 layer. The first is for
the input sequence feeding into this layer, and the second is for the
previous hidden units feeding into the layer.
The output layer has a single unit for the response.


We  fit the model in a similar fashion to previous networks. We
supply the `fit` function with test data as validation data, so that when
we monitor its progress and plot the history function we can see the
progress on the test data. Of course we should not use this as a basis for
early stopping, since then the test performance would be biased.


```{r}
history <- model %>% fit(
    xrnn[istrain,, ], arframe[istrain, "log_volume"],
#    batch_size = 64, epochs = 200,
    batch_size = 64, epochs = 75,
    validation_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
kpred <- predict(model, xrnn[!istrain,, ])
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
```

This model takes about one minute to train.

We could replace the  `keras_model_sequential()`  command above with the following command:

```{r}
model <- keras_model_sequential() %>%
   layer_flatten(input_shape = c(5, 3)) %>%
   layer_dense(units = 1)
```

Here, `layer_flatten()` simply takes the input sequence and
turns it into a long vector of predictors. This results in a linear AR model.
To fit a nonlinear AR model, we could add in a hidden layer.

However, since we already have the matrix of lagged variables from the AR
model that we fit earlier using the `lm()` command, we can actually fit a nonlinear AR model without needing to perform flattening.
We extract the model matrix `x` from `arframed`, which
includes the `day_of_week` variable.

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

The `-1` in the formula avoids the creation of a column of ones for
the intercept. The variable `day\_of\_week` is a five-level
factor (there are five trading days), and the
 `-1` results in  five rather than four dummy variables.

The rest of the steps to fit a nonlinear AR model should by now be familiar.

```{r}
arnnd <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu',
      input_shape = ncol(x)) %>%
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
    optimizer = optimizer_rmsprop())
history <- arnnd %>% fit(
#    x[istrain, ], arframe[istrain, "log_volume"], epochs = 100, 
    x[istrain, ], arframe[istrain, "log_volume"], epochs = 30, 
    batch_size = 32, validation_data =
      list(x[!istrain, ], arframe[!istrain, "log_volume"])
  )
plot(history)
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```



