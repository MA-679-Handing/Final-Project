---
title: "RNN_Model"
author: "Handing Zhang"
date: "4/25/2022"
output: pdf_document
---
Here I will fit a RNN model 

```{r Import Data. Load Packages}
source("Data_Wrangling.R")
pacman::p_load(ISLR2,keras,tidyverse,magrittr,ggplot2)
Z_416 <- Z_416 %>% select(-c(Y1, Y2))

set.seed(100)
```




We first create lagged versions of the time series.  We start with a function that takes as input a data
matrix and a lag $L$, and returns a lagged version of the matrix. It
simply inserts $L$ rows of `NA` at the top, and truncates the
bottom.

```{r, a function that takes dataframe x and spit it with k rows of lags. (Intuitively the dataframe moved downwards by k steps with NA rows as its foot prints.)}
lagm <- function(x, k = 1) {
   n <- nrow(x)
   pad <- matrix(NA, k, ncol(x))
   colnames(pad) <- colnames(x)
   rbind(pad, x[1:(n - k), ])
}


# To Help understanding ####
# a <- matrix(c(rep(1,6), rep(2,6), rep(3,6), rep(4, 6), rep(5, 6), rep(6,6) ), nrow = 6, byrow = T)
# a
# lagm(a,2)
# data.frame(c(),a,a)
# aaaaa <- data.frame(letters[1:6],
#    L1 = lagm(a, 1), L2 = lagm(a, 2),
#    L3 = lagm(a, 3), L4 = lagm(a, 4),
#    L5 = lagm(a, 5)
#  )
# aaaaa
# a
```

We now use this function to create a data frame with all the required
lags, as well as the response variable.

```{r a Function that combines L dataframes and our response Y. Each dataframe i takes i lags(defined from the function above.)}

# Make sure the input dataframe contains column "Y", our response!
lag_gene <- function(df, L){
  temp <- data.frame(Y = df[,"Y"])
  for(i in c(1:L)){
    # name <- paste0("L", i)
    # assign(name, lagm(df, i))
    temp <- data.frame(temp, lagm(df, i)[,-1])
  }
  return(temp)
}

```


```{r create a list containing data frame with different number of steps of lags. We choose the range of 1 to 20 steps lags}

lag_list_gene <- function(df){
  Lag_List <- list()
  for (i in c(1:20)){
  Lag_List[[i]] <- lag_gene(df, i)[-c(1:i),] ## here we use index to get rid of NA created by above steps.
  }
  return(Lag_List)
}

Z_416_AR_List <- lag_list_gene(Z_416)

## to demonstrate.
dim(Z_416_AR_List[[1]]) #1 step back to predict
dim(Z_416_AR_List[[2]]) #2 steps back to predict
dim(Z_416_AR_List[[3]]) #3 steps back to predict
dim(Z_416_AR_List[[4]]) #4 steps back to predict
# As i goes up.
# each time we delete one more row since one more row contains NA.
# each time we have 26 more features. the neurons from one more time step backwards.
```



```{r Logistic AR model. predict on the last 20% of the test data.}
## A function that fits AR model for the data with number of steps we look back ranging from 1 to 20 and compare them in terms of testing accuracy with the last 20% of time.

AR_logistic <- function(df){
  acc <- c()
  df <- lag_list_gene(df)
    for (i in c(1:20)){
      
    # Prepare the training set(first 80% of rows) and testing set(the rest)
      train_ind <- c(1:floor( 0.8 * nrow(df[[i]]) )  )
      train <- df[[i]][train_ind, ]
      test <- df[[i]][-train_ind, ]
      
      arfit <- glm(Y ~ ., data = train, family = "binomial")
      arpred <- predict(arfit, test, type = "response")
      arpred <- ifelse(arpred > 0.5, 1, 0) # The threshold might be improved by methods like ROC.
      acc <- append(acc ,mean(arpred == test[, "Y"] ))
    }
  return(acc)
} 
# This iterating fitting process takes around 1 minute on my laptop.
Z_416_AR <-  as.data.frame(cbind(acc = AR_logistic(Z_416), steps = c(1:20))) # 50s
Z_416_AR

# Plot the relationship
Z_416_AR %>% 
  ggplot(aes(x = steps, y = acc)) + 
  geom_point() + 
  geom_line() +
  labs(title = "Prediction Accuracy vs Number of steps back", subtitle = "Z_416")

## I try to fit the model for other larger mice datasets. But the running time is too long. I temporarily forfeit that plan.
# AR_logistic(Z_412) 
# AR_logistic(Z_414)
# AR_logistic(Z_417)
```
oo

The last two lines compute the $R^2$ on the test data, as defined in (3.17).

We refit this model, including the factor variable `day_of_week`.


To fit the RNN, we need to reshape these data, since it expects a
sequence of $L=5$ feature vectors $X=\{X_\ell\}_1^L$ for each observation, as in (10.20) on
page  428. These are  lagged versions of the
time series going back $L$ time points.

```{r Here I will pick the 5 steps lagged dataframe to fit the model}


n <- nrow(Z_416_AR_List[[5]])
xrnn <- data.matrix(Z_416_AR_List[[5]][, -1])
xrnn <- array(xrnn, c(n, ncol(Z_416_AR_List[[5]]) - 1, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)
```

We have done this in four steps. The first simply extracts the
$n\times 15$ matrix of lagged versions of the three predictor
variables from `arframe`. The second converts this matrix to a
$n\times 3\times 5$ array. We can do this by simply changing the
dimension attribute, since the new array is filled column wise. The
third step reverses the order of lagged variables, so that index $1$
is furthest back in time, and index $5$ closest. The
final step rearranges the coordinates of the array (like a partial
transpose) into the format that the RNN module in `keras`
expects.

Now we are ready to proceed with the RNN, which uses 12 hidden units.

```{r}
dim(Z_416_AR_List[[5]])


model <- keras_model_sequential() %>%
   layer_simple_rnn(units = 5, ## number of steps we look back
      input_shape = list(5,ncol(Z_416_AR_List[[5]]) - 1 ),
      dropout = 0.1, recurrent_dropout = 0.1) %>%
   layer_lstm(units = 32) %>%
   layer_dense(units = 1, activation = "sigmoid")

model %>% compile(optimizer = optimizer_rmsprop(),
    loss = "binary_crossentropy")
```
 ############################################
 
```{r}
model2 <- keras_model_sequential() %>%
   layer_simple_rnn(units = 5, ## number of steps we look back
      input_shape = list(5,ncol(Z_416_AR_List[[5]]) - 1 ),
      dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer_dense(units = 1, activation = "sigmoid")
  
model2 %>% compile(optimizer = optimizer_rmsprop(),
    loss = "binary_crossentropy")

model2 %>% compile(loss = 'binary_crossentropy', 
                  optimizer = 'RMSprop', 
                  metrics = c('accuracy'))



# 
# > model <- keras_model_sequential() %>%
# + layer_embedding(input_dim = 10000, output_dim = 32) %>%
# + layer_lstm(units = 32) %>%
# + layer_dense(units = 1, activation = "sigmoid")
# 

```

We specify two forms of dropout for the units feeding into the  hidden
 layer. The first is for
the input sequence feeding into this layer, and the second is for the
previous hidden units feeding into the layer.
The output layer has a single unit for the response.


We  fit the model in a similar fashion to previous networks. We
supply the `fit` function with test data as validation data, so that when
we monitor its progress and plot the history function we can see the
progress on the test data. Of course we should not use this as a basis for
early stopping, since then the test performance would be biased.


```{r}
train_ind <- c(1:floor( 0.8 * nrow(Z_416_AR_List[[5]]) ) )

history <- model2 %>% fit(
    xrnn[train_ind,, ], Z_416_AR_List[[5]][train_ind, "Y"],
#    batch_size = 64, epochs = 200,
    batch_size = 64, epochs = 75,
    validation_data =
      list(xrnn[-train_ind,, ], Z_416_AR_List[[5]][-train_ind, "Y"])
  )
kpred <- predict(model2, xrnn[-train_ind,, ])
kpred

## How do I make 0 1 outcome by setting arguments in the layer.
## How are AR model and RNN connected.
## In terms of training index of time series data.
## 
```

This model takes about one minute to train.

We could replace the  `keras_model_sequential()`  command above with the following command:

```{r}
model <- keras_model_sequential() %>%
   layer_flatten(input_shape = c(5, 3)) %>%
   layer_dense(units = 1)
```

Here, `layer_flatten()` simply takes the input sequence and
turns it into a long vector of predictors. This results in a linear AR model.
To fit a nonlinear AR model, we could add in a hidden layer.

However, since we already have the matrix of lagged variables from the AR
model that we fit earlier using the `lm()` command, we can actually fit a nonlinear AR model without needing to perform flattening.
We extract the model matrix `x` from `arframed`, which
includes the `day_of_week` variable.

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

The `-1` in the formula avoids the creation of a column of ones for
the intercept. The variable `day\_of\_week` is a five-level
factor (there are five trading days), and the
 `-1` results in  five rather than four dummy variables.

The rest of the steps to fit a nonlinear AR model should by now be familiar.

```{r}
arnnd <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu',
      input_shape = ncol(x)) %>%
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
    optimizer = optimizer_rmsprop())
history <- arnnd %>% fit(
#    x[istrain, ], arframe[istrain, "log_volume"], epochs = 100, 
    x[istrain, ], arframe[istrain, "log_volume"], epochs = 30, 
    batch_size = 32, validation_data =
      list(x[!istrain, ], arframe[!istrain, "log_volume"])
  )
plot(history)
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```
